package parse

import (
	"io"
	"iter"
	"sync"

	"github.com/alecthomas/participle/v2"
	"github.com/alecthomas/participle/v2/lexer"

	"github.com/ardnew/envmux/pkg"
)

//nolint:gochecknoglobals
var (
	// XX matches comments and whitespace emitted to the parser.
	//
	// These tokens are not included in the AST.
	//
	// This may change in the future to support semantic comments
	// such as documentation, metadata, or runtime directives.
	XX = `(?:/\*(?:[^*]|\*[^/])*\*/|(?://|#)[^\r\n]*\r?\n|\s)`

	// NS matches a namespace identifier.
	//
	// Namespace identifiers can be virtually any string that does not
	// contain '\t', '\r', '\n', or syntactical punctuation.
	//
	// Spaces are allowed within the identifier, but all surrounding
	// whitespace is ignored implicitly by rule precedence.
	ns = `[^(){}\[\]<>,;=\s]`
	NS = ns + `+(?: +` + ns + `|` + ns + `)*`

	// ID matches a conventional shell identifier.
	//
	// It is used to capture environment variable identifiers.
	ID = `[a-zA-Z_][a-zA-Z0-9_]*`

	// QQ matches a double-quoted string.
	//
	// It is used to capture string literals.
	QQ = `"(?:\\.|[^"])*"`

	// NU matches a numeric literal.
	//
	// It is used to capture integer and floating-point literals.
	//
	// Numeric literals may be prefixed with '+' or '-' to indicate their sign.
	//
	// Several different prefix conventions can be used to express integer
	// literals in common bases:
	//
	//  - Binary: "0b"
	//  - Octal: "0" or "0o"
	//  - Hexadecimal: "0x", "\x", or "$" (‚ù§ Pascal)
	//
	// Floating-point literals can only be expressed in decimal form,
	// but may also include a decimal exponent with optional sign
	// following a suffixed 'e' or 'E'.
	NU = `[+-]?(?:(?:[0\\]x|\$)[0-9a-fA-F]+|0o?[0-7]+|0b[01]+|(?:[0-9]+\.?[0-9]*|\.[0-9]+)(?:[eE][+-]?[0-9]+)?)`

	// EX matches an expression.
	//
	// The expression grammar is defined by [expr-lang]. This lexer needs to
	// capture everything that may be part of a single [expr-lang] expression.
	//
	// For example, `x = 1 + 2;` would capture `1 + 2` as a single token, and:
	//
	//     x = {
	//       let x="foo"; x+user.Name
	//     } | upper()
	//
	// captures `{\n  let x="foo"; x+user.Name\n} | upper()` as a single token.
	//
	// We use the pattern "." so that the lexer captures any possible symbol and
	// then rely on a custom parser to consume from it until it reaches the first
	// unscoped record separator (RS) or end of line (EOL) symbol.
	//
	// By unscoped, we mean that the RS or EOL is not contained within any
	// enclosing braces or parentheses. All scoped RS and EOL symbols are treated
	// as part of the expression.
	//
	// Unbalanced braces or parentheses are recognized as invalid input and will
	// result in an error while parsing the namespace definition.
	//
	// The pattern `\\.` is used to capture escape sequences in the expression as
	// individual tokens, so that they are not counted when matching balanced
	// braces or parentheses.
	//
	// See type [Expr] and method [Expr.Parse], which implements interface
	// [participle.Parseable].
	//
	// [expr-lang]: https://github.com/expr-lang/expr
	EX = `(?:\\.|.)?`

	FS = `,` // FS matches a field separator.
	RS = `;` // RS matches a record separator.

	// AA matches any possibly-escaped non-whitespace symbol.
	//
	// Normally, it is an unrecoverable fatal error when the lexer encounters
	// unrecognized symbols.
	//
	// This pattern enables the lexer to emit tokens to the parser that are not
	// strictly recognized by its grammar and would otherwise fail to consume.
	//
	// This is useful for a few reasons:
	//
	//  - The parser can use these tokens to provide better errors/diagnostics.
	//  - The parser can choose to accept these tokens in special cases.
	//  - The parser can recover invalid input by ignoring or translating tokens.
	AA = `[^\s]`

	// EE is the escaped version of AA and is used to capture escape sequences.
	EE = `\\` + AA
)

// LexerGenerator is used internally to generate lexer.go, which provides
// the concrete implementation of [ConfigLexer].
//
// It is generated by running "go generate" in the same directory as this file.
// You must regenerate lexer.go if you change the rules in this file.
//
// Note that this generates a greedy lexer.
// It is important to consider what this implies.
//
// In particular, if the lexer falls to a rule with lesser precedence,
// and it scans input that would have matched a rule with greater precedence,
// the greater rule will never match.
//
// The input can then only be consumed by a rule with lesser precedence.
// If no such rule exists, the input is invalid and will not be parsed.
//
//go:generate bash internal/lexer.bash
var LexerGenerator = sync.OnceValue( //nolint:gochecknoglobals
	func() *lexer.StatefulDefinition {
		return lexer.MustStateful(lexer.Rules{
			`Root`: {
				lexer.Include(`Elidable`),
				{Name: `NS`, Pattern: NS, Action: lexer.Push(`Namespace`)},
			},

			`Namespace`: {
				lexer.Include(`Elidable`),
				{Name: `CO`, Pattern: `<`, Action: lexer.Push(`Composite`)},
				{Name: `PO`, Pattern: `\(`, Action: lexer.Push(`Parameter`)},
				{Name: `SO`, Pattern: `{`, Action: lexer.Push(`Statement`)},
				lexer.Return(),
			},

			`Composite`: {
				lexer.Include(`Elidable`),
				{Name: `CC`, Pattern: `>`, Action: lexer.Pop()},
				{Name: `FS`, Pattern: FS, Action: nil},
				{Name: `NS`, Pattern: NS, Action: nil},
			},

			`Parameter`: {
				lexer.Include(`Elidable`),
				{Name: `PC`, Pattern: `\)`, Action: lexer.Pop()},
				{Name: `FS`, Pattern: FS, Action: nil},
				{Name: `QQ`, Pattern: QQ, Action: nil},
				{Name: `NU`, Pattern: NU, Action: nil},
				{Name: `ID`, Pattern: ID, Action: nil},
			},

			`Statement`: {
				lexer.Include(`Elidable`),
				{Name: `SO`, Pattern: `{`, Action: lexer.Push(`Statement`)},
				{Name: `SC`, Pattern: `}`, Action: lexer.Pop()},
				{Name: `RS`, Pattern: RS, Action: nil},
				{Name: `ID`, Pattern: ID, Action: nil},
				{Name: `OP`, Pattern: `=`, Action: nil},
				{Name: `EX`, Pattern: EX, Action: nil},
			},

			`Elidable`:  {{Name: `XX`, Pattern: XX, Action: nil}},
			`Printable`: {{Name: `AA`, Pattern: AA, Action: nil}},
			`Escaped`:   {{Name: `EE`, Pattern: EE, Action: nil}},
		})
	},
)

// AST is the root node of the parsed configuration file.
//
// It consumes tokens from the `Root` state of the lexer.
type AST struct {
	Pos    lexer.Position // Pos records the start position of the node.
	EndPos lexer.Position // EndPos records the end position of the node.
	Tokens []lexer.Token  // Tokens records the tokens consumed by the node

	List []*Namespace `parser:"( @@ XX* )*"`
}

// Namespace associates a composition of environment variable definitions with
// a namespace identifier. Variable definitions are expressed using the entire
// [expr-lang] grammar.
//
// [expr-lang]: https://github.com/expr-lang/expr
type Namespace struct {
	Pos    lexer.Position // Pos records the start position of the node.
	EndPos lexer.Position // EndPos records the end position of the node.
	Tokens []lexer.Token  // Tokens records the tokens consumed by the node

	Name string       `parser:"XX* @NS"`
	Def  string       `parser:"XX* ("`
	Com  []*Composite `parser:"( XX* CO XX* ( @@ ( XX* FS @@ )* )? XX* CC )?"`
	Par  []*Parameter `parser:"( XX* PO XX* ( @@ ( XX* FS @@ )* )? XX* PC )?"`
	Sta  []*Statement `parser:"( XX* SO XX* ( @@   XX*          )* XX* SC )?"`
	End  string       `parser:"XX* )!"`
}

// Composite represents a composition node in the AST.
type Composite struct {
	Pos    lexer.Position // Pos records the start position of the node.
	EndPos lexer.Position // EndPos records the end position of the node.
	Tokens []lexer.Token  // Tokens records the tokens consumed by the node.

	ID string `parser:"@NS"`
}

// Parameter represents a parameter node in the AST.
type Parameter struct {
	Pos    lexer.Position // Pos records the start position of the node.
	EndPos lexer.Position // EndPos records the end position of the node.
	Tokens []lexer.Token  // Tokens records the tokens consumed by the node

	ID string `parser:"@( QQ | NU | ID )"`
}

// Statement represents a statement node in the AST.
//
// It assigns or amends value to a variable identifier in a namespace.
// The value can be a literal or an evaluated expression.
//
// Expressions are evaluated in the context of the enclosing namespace
// and the implicit parameter (identified with [vars.ParameterKey])
// for each parameter to the namespace.
//
// Each parameter's evaluation is assigned to the variable based on the formal
// syntax used by the parameter.
type Statement struct {
	Pos    lexer.Position // Pos records the start position of the node.
	EndPos lexer.Position // EndPos records the end position of the node.
	Tokens []lexer.Token  // Tokens records the tokens consumed by the node

	ID string `parser:"@ID"`
	Op string `parser:"XX* @OP"`
	Ex *Expr  `parser:"XX* @@"`
}

// Options are the default participle options used to build the parser.
//
//nolint:gochecknoglobals
var Options = []participle.Option{participle.Lexer(ConfigLexer)}

// ParseOptions are the default participle parse options used to parse the AST.
//
//nolint:gochecknoglobals
var ParseOptions = []participle.ParseOption{participle.AllowTrailing(true)}

func TraceOptions(w io.Writer) []participle.ParseOption {
	opt := make([]participle.ParseOption, len(ParseOptions)+1)

	copy(opt, ParseOptions)
	opt[len(ParseOptions)] = participle.Trace(w)

	return opt
}

// Compositions returns a sequence of unique namespace identifiers composing
// the Namespace.
// Duplicate names are removed; only the first occurrence is retained.
func (s *Namespace) Compositions() iter.Seq[string] {
	if s == nil {
		return nil
	}

	unique := make(pkg.Unique[string])

	return func(yield func(string) bool) {
		for _, v := range s.Com {
			if unique.Set(v.ID) && !yield(v.ID) {
				return
			}
		}
	}
}

// Parameters returns a sequence of unique parameters from the [Namespace]
// appended with the given arguments.
// The additional names are appended to the namespace's [Parameter] slice.
// Duplicate names are removed; only the first occurrence is retained.
func (s *Namespace) Parameters(appends ...string) iter.Seq[string] {
	if s == nil {
		return nil
	}

	unique := make(pkg.Unique[string])

	return func(yield func(string) bool) {
		for _, v := range s.Par {
			if unique.Set(v.ID) && !yield(v.ID) {
				return
			}
		}

		for _, append := range appends {
			if unique.Set(append) && !yield(append) {
				return
			}
		}
	}
}

// build returns a singleton parser for the AST type.
//
//nolint:gochecknoglobals
var build = sync.OnceValue(
	func() *participle.Parser[AST] {
		return participle.MustBuild[AST](Options...)
	},
)

// EBNF returns the parser grammar as a string in Extended Backus-Naur Form.
func EBNF() string { return build().String() }

// Make returns a function that parses an [AST] from the provided [io.Reader].
func Make(r io.Reader) func() (*AST, error) {
	return func() (*AST, error) {
		ast, err := build().Parse(pkg.Name, r, ParseOptions...)
		if err != nil {
			return nil, err
		}

		return ast, nil
	}
}
