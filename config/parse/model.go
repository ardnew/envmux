package parse

import (
	"io"
	"iter"
	"sync"

	"github.com/alecthomas/participle/v2"
	"github.com/alecthomas/participle/v2/lexer"

	"github.com/ardnew/envmux/pkg"
)

//nolint:gochecknoglobals
var (
	// XS matches comments and whitespace emitted to the parser.
	//
	// These tokens are not included in the AST.
	//
	// This may change in the future to support semantic comments
	// such as documentation, metadata, or runtime directives.
	XS = `(?:/\*(?:[^*]|\*[^/])*\*/|(?://|#)[^\r\n]*\r?\n|\s+)`

	// NS matches a namespace identifier.
	//
	// Namespace identifiers can be virtually any string that does not
	// contain '\t', '\r', '\n', or syntactical punctuation.
	//
	// Spaces are allowed within the identifier, but all surrounding
	// whitespace is ignored implicitly by rule precedence.
	NS = `[^(){}\[\]<>,;=\s]+(?: +[^(){}\[\]<>,;=\s]+)*`

	// ID matches a conventional shell identifier.
	//
	// It is used to capture environment variable identifiers.
	ID = `[a-zA-Z_][a-zA-Z0-9_]*`

	// QQ matches a double-quoted string.
	//
	// It is used to capture string literals.
	QQ = `"(?:\\.|[^"])*"`

	// NU matches a numeric literal.
	//
	// It is used to capture integer and floating-point literals.
	//
	// Numeric literals may be prefixed with '+' or '-' to indicate their sign.
	//
	// Several different prefix conventions can be used to express integer
	// literals in common bases:
	//
	//  - Binary: "0b"
	//  - Octal: "0" or "0o"
	//  - Hexadecimal: "0x", "\x", or "$" (‚ù§ Pascal)
	//
	// Floating-point literals can only be expressed in decimal form,
	// but may also include a decimal exponent with optional sign
	// following a suffixed 'e' or 'E'.
	NU = `[+-]?(?:(?:[0\\]x|\$)[0-9a-fA-F]+|0o?[0-7]+|0b[01]+|(?:[0-9]+\.?[0-9]*|\.[0-9]+)(?:[eE][+-]?[0-9]+)?)`

	// EX matches an expression.
	//
	// The expression grammar is defined by [expr-lang]. This lexer needs to
	// capture everything that may be part of a single [expr-lang] expression.
	//
	// For example, `x = 1 + 2;` would capture `1 + 2` as a single token, and:
	//
	//     x = {
	//       let x="foo"; x+user.Name
	//     } | upper()
	//
	// captures `{\n  let x="foo"; x+user.Name\n} | upper()` as a single token.
	//
	// We use the pattern "." so that the lexer captures any possible symbol and
	// then rely on a custom parser to consume from it until it reaches the first
	// unscoped record separator (RS) or end of line (EOL) symbol.
	//
	// By unscoped, we mean that the RS or EOL is not contained within any
	// enclosing braces or parentheses. All scoped RS and EOL symbols are treated
	// as part of the expression.
	//
	// Unbalanced braces or parentheses are recognized as invalid input and will
	// result in an error while parsing the namespace definition.
	//
	// The pattern `\\.` is used to capture escape sequences in the expression as
	// individual tokens, so that they are not counted when matching balanced
	// braces or parentheses.
	//
	// See type [Expr] and method [Expr.Parse], which implements interface
	// [participle.Parseable].
	//
	// [expr-lang]: https://github.com/expr-lang/expr
	EX = `\\.|.`

	FS = `,` // FS matches a field separator.
	RS = `;` // RS matches a record separator.
)

// LexerGenerator is used internally to generate lexer.go, which provides
// the concrete implementation of [ConfigLexer].
//
// It is generated by running "go generate" in the same directory as this file.
// You must regenerate lexer.go if you change the rules in this file.
//
// Note that this generates a greedy lexer.
// It is important to consider what this implies.
//
// In particular, if the lexer falls to a rule with lesser precedence,
// and it scans input that would have matched a rule with greater precedence,
// the greater rule will never match.
//
// The input can then only be consumed by a rule with lesser precedence.
// If no such rule exists, the input is invalid and will not be parsed.
//
//go:generate bash internal/lexer.bash
var LexerGenerator = sync.OnceValue( //nolint:gochecknoglobals
	func() *lexer.StatefulDefinition {
		return lexer.MustStateful(lexer.Rules{
			`Global`: {
				{Name: `XS`, Pattern: XS, Action: nil},
			},
			`Ignore`: {
				{Name: `RS`, Pattern: RS, Action: nil},
			},
			`Root`: {
				lexer.Include(`Global`),
				{Name: `NS`, Pattern: NS, Action: lexer.Push(`Definition`)},
				lexer.Include(`Ignore`),
			},
			`Definition`: {
				lexer.Include(`Global`),
				{Name: `CO`, Pattern: `<`, Action: lexer.Push(`Composite`)},
				{Name: `PO`, Pattern: `\(`, Action: lexer.Push(`Parameter`)},
				{Name: `SO`, Pattern: `{`, Action: lexer.Push(`Statement`)},
				lexer.Return(),
			},
			`Composite`: {
				lexer.Include(`Global`),
				{Name: `CC`, Pattern: `>`, Action: lexer.Pop()},
				{Name: `FS`, Pattern: FS, Action: nil},
				{Name: `NS`, Pattern: NS, Action: nil},
			},
			`Parameter`: {
				lexer.Include(`Global`),
				{Name: `PC`, Pattern: `\)`, Action: lexer.Pop()},
				{Name: `FS`, Pattern: FS, Action: nil},
				{Name: `QQ`, Pattern: QQ, Action: nil},
				{Name: `NU`, Pattern: NU, Action: nil},
				{Name: `ID`, Pattern: ID, Action: nil},
			},
			`Statement`: {
				lexer.Include(`Global`),
				{Name: `SO`, Pattern: `{`, Action: lexer.Push(`Statement`)},
				{Name: `SC`, Pattern: `}`, Action: lexer.Pop()},
				{Name: `RS`, Pattern: RS, Action: nil},
				{Name: `ID`, Pattern: ID, Action: nil},
				{Name: `OP`, Pattern: `=`, Action: nil},
				{Name: `EX`, Pattern: EX, Action: nil},
			},
		})
	},
)

// Composite represents a composition node in the AST.
type Composite struct {
	Pos    lexer.Position // Pos records the start position of the node.
	EndPos lexer.Position // EndPos records the end position of the node.
	Tokens []lexer.Token  // Tokens records the tokens consumed by the node.

	ID string `parser:"XS* @NS"`
}

// Parameter represents a parameter node in the AST.
type Parameter struct {
	Pos    lexer.Position // Pos records the start position of the node.
	EndPos lexer.Position // EndPos records the end position of the node.
	Tokens []lexer.Token  // Tokens records the tokens consumed by the node

	ID string `parser:"XS* @( QQ | NU | ID )"`
}

// Statement represents a statement node in the AST.
// It assigns or amends value to a variable identifier in a namespace.
// The value can be a literal or an evaluated expression.
// Expressions are evaluated in the context of the enclosing namespace
// and the implicit parameter (identified with [vars.ParameterKey])
// for each parameter to the namespace.
// Each parameter's evaluation is assigned to the variable based on the formal
// syntax used by the parameter.
type Statement struct {
	Pos    lexer.Position // Pos records the start position of the node.
	EndPos lexer.Position // EndPos records the end position of the node.
	Tokens []lexer.Token  // Tokens records the tokens consumed by the node

	ID string `parser:"XS* @ID"`
	Op string `parser:"XS* @OP"`
	Ex *Expr  `parser:"XS* @@"`
}

// Definition represents a namespace definition node in the AST.
type Definition struct {
	Pos    lexer.Position // Pos records the start position of the node.
	EndPos lexer.Position // EndPos records the end position of the node.
	Tokens []lexer.Token  // Tokens records the tokens consumed by the node

	Com []*Composite `parser:"( XS* CO ( @@ ( XS* FS @@ )* )? XS* CC )?"`
	Par []*Parameter `parser:"( XS* PO ( @@ ( XS* FS @@ )* )? XS* PC )?"`
	Sta []*Statement `parser:"( XS* SO ( @@ ( XS* RS @@ )* )? XS* SC )?"`
}

// Namespace associates a composition of environment variable definitions with
// a namespace identifier.
type Namespace struct {
	Pos    lexer.Position // Pos records the start position of the node.
	EndPos lexer.Position // EndPos records the end position of the node.
	Tokens []lexer.Token  // Tokens records the tokens consumed by the node

	Name string      `parser:"XS* @NS"`
	Spec *Definition `parser:"XS* @@!"`
}

// AST is the root node of the parsed configuration file.
type AST struct {
	Pos    lexer.Position // Pos records the start position of the node.
	EndPos lexer.Position // EndPos records the end position of the node.
	Tokens []lexer.Token  // Tokens records the tokens consumed by the node

	List []*Namespace `parser:"( @@ ( XS | RS )* )*"`
}

// Options are the default participle options used to build the parser.
//
//nolint:gochecknoglobals
var Options = []participle.Option{participle.Lexer(ConfigLexer)}

// ParseOptions are the default participle parse options used to parse the AST.
//
//nolint:gochecknoglobals
var ParseOptions = []participle.ParseOption{participle.AllowTrailing(true)}

func TraceOptions(w io.Writer) []participle.ParseOption {
	opt := make([]participle.ParseOption, len(ParseOptions)+1)

	copy(opt, ParseOptions)
	opt[len(ParseOptions)] = participle.Trace(w)

	return opt
}

// build returns a singleton parser for the AST type.
//
//nolint:gochecknoglobals
var build = sync.OnceValue(
	func() *participle.Parser[AST] {
		return participle.MustBuild[AST](Options...)
	},
)

// Grammar returns the grammar of the parser as a string in EBNF format.
func Grammar() string { return build().String() }

// Load parses the input reader and returns the AST.
func Load(r io.Reader) (*AST, error) {
	return build().Parse(pkg.Name, r, ParseOptions...)
}

// Compositions returns a sequence of unique namespace identifiers composing
// the Definition.
// Duplicate names are removed; only the first occurrence is retained.
func (s *Definition) Compositions() iter.Seq[string] {
	if s == nil {
		return nil
	}

	unique := make(pkg.Unique[string])

	return func(yield func(string) bool) {
		for _, v := range s.Com {
			if unique.Set(v.ID) && !yield(v.ID) {
				return
			}
		}
	}
}

// Parameters returns a sequence of unique parameters from the [Definition]
// appended with the given arguments.
// The additional names are appended to the definition's subjects.
// Duplicate names are removed; only the first occurrence is retained.
func (s *Definition) Parameters(appends ...string) iter.Seq[string] {
	if s == nil {
		return nil
	}

	unique := make(pkg.Unique[string])

	return func(yield func(string) bool) {
		for _, v := range s.Par {
			if unique.Set(v.ID) && !yield(v.ID) {
				return
			}
		}

		for _, append := range appends {
			if unique.Set(append) && !yield(append) {
				return
			}
		}
	}
}
