package parse

import (
	"io"
	"strconv"
	"sync"
	"unicode/utf8"

	"github.com/alecthomas/participle/v2"
	"github.com/alecthomas/participle/v2/lexer"

	"github.com/ardnew/envmux/pkg"
)

//nolint:gochecknoglobals
var (
	// XX matches comments and whitespace emitted to the parser.
	//
	// These tokens are not included in the AST.
	//
	// This may change in the future to support semantic comments
	// such as documentation, metadata, or runtime directives.
	XX = `(?:/\*+(?:[^*]|\*[^*/])*\*+/|(?://|#)[^\r\n]*\r?\n|\s+)`

	FS = `,`  // FS matches a field separator.
	RS = `;`  // RS matches a record separator.
	NL = `\n` // NL matches a newline character.

	co, cc, cp = `<`, `>`, `\` + co + `\` + cc
	so, sc, sp = `{`, `}`, `\` + so + `\` + sc
	po, pc, pp = `(`, `)`, `\` + po + `\` + pc
	ao, ac, ap = `[`, `]`, `\` + ao + `\` + ac
	bp         = cp + sp + pp + ap

	// NS matches a namespace identifier.
	//
	// Namespace identifiers can be virtually any string that does not
	// contain '\t', '\r', '\n', or syntactical punctuation.
	//
	// Spaces are allowed within the identifier, but all surrounding
	// whitespace is ignored implicitly per rule precedence.
	NS = `\b(?:[^#/` + bp + `,;=\s]|/[^*/])+(?: +(?:[^#/` + bp + `,;=\s]|/[^*/])+)*\b`

	// ID matches a conventional shell identifier.
	//
	// It is used to capture environment variable identifiers.
	ID = `[a-zA-Z_][a-zA-Z0-9_]*`

	// QQ matches a double-quoted string.
	//
	// It is used to capture string literals.
	QQ = `"(?:\\.|[^"])*"`

	// NU matches a numeric literal.
	//
	// It is used to capture integer and floating-point literals.
	//
	// Numeric literals may be prefixed with '+' or '-' to indicate their sign.
	//
	// Several different prefix conventions can be used to express integer
	// literals in common bases:
	//
	//  - Binary: "0b"
	//  - Octal: "0" or "0o"
	//  - Hexadecimal: "0x", "\x", or "$" (❤ Pascal)
	//
	// Floating-point literals can only be expressed in decimal form,
	// but may also include a decimal exponent with optional sign
	// following a suffixed 'e' or 'E'.
	NU = `[+-]?(?:(?:[0\\]x|\$)[0-9a-fA-F]+|0o?[0-7]+|0b[01]+|(?:[0-9]+\.?[0-9]*|\.[0-9]+)(?:[eE][+-]?[0-9]+)?)`

	// EX matches an expression.
	//
	// The expression grammar is defined by [expr-lang]. This lexer needs to
	// capture everything that may be part of a single [expr-lang] expression.
	//
	// For example, `x = 1 + 2;` would capture `1 + 2` as a single token, and:
	//
	//     x = {
	//       let x="foo"; x+user.Name
	//     } | upper()
	//
	// captures `{\n  let x="foo"; x+user.Name\n} | upper()` as a single token.
	//
	// We use the pattern "." so that the lexer captures any possible symbol and
	// then rely on a custom parser to consume from it until it reaches the first
	// unscoped record separator (RS) or end of line (EOL) symbol.
	//
	// By unscoped, we mean that the RS or EOL is not contained within any
	// enclosing braces or parentheses. All scoped RS and EOL symbols are treated
	// as part of the expression.
	//
	// Unbalanced braces or parentheses are recognized as invalid input and will
	// result in an error while parsing the namespace definition.
	//
	// The pattern `\\.` is used to capture escape sequences in the expression as
	// individual tokens, so that they are not counted when matching balanced
	// braces or parentheses.
	//
	// See type [Expr] and method [Expr.Parse], which implements interface
	// [participle.Parseable].
	//
	// [expr-lang]: https://github.com/expr-lang/expr
	EX = `(?:\\.|.)?`

	// AA matches any possibly-escaped non-whitespace symbol.
	//
	// Normally, it is an unrecoverable fatal error when the lexer encounters
	// unrecognized symbols.
	//
	// This pattern enables the lexer to emit tokens to the parser that are not
	// strictly recognized by its grammar and would otherwise fail to consume.
	//
	// This is useful for a few reasons:
	//
	//  - The parser can use these tokens to provide better errors/diagnostics.
	//  - The parser can choose to accept these tokens in special cases.
	//  - The parser can recover invalid input by ignoring or translating tokens.
	AA = `[^\s]`

	// EE is the escaped version of AA and is used to capture escape sequences.
	EE = `\\` + AA

	// Z0 matches no valid symbol and is used to capture invalid input.
	//
	// The \A anchor matches the start of a string.
	// So, by definition, it can never match following a valid symbol.
	//
	// Any string at all will suffice — "nevermatch" is used for readability.
	Z0 = `nevermatch\A`
	ZZ = string(utf8.RuneError)
)

// LexerGenerator is used internally to generate lexer.go, which provides
// the concrete implementation of [ConfigLexer].
//
// It is generated by running "go generate" in the same directory as this file.
// You must regenerate lexer.go if you change the rules in this file.
//
// Note that this generates a greedy lexer.
// It is important to consider what this implies.
//
// In particular, if the lexer falls to a rule with lesser precedence,
// and it scans input that would have matched a rule with greater precedence,
// the greater rule will never match.
//
// The input can then only be consumed by a rule with lesser precedence.
// If no such rule exists, the input is invalid and will not be parsed.
//
//go:generate bash internal/lexer.bash
var LexerGenerator = sync.OnceValue( //nolint:gochecknoglobals
	func() *lexer.StatefulDefinition {
		return lexer.MustStateful(lexer.Rules{
			`Global`:    {{Name: `XX`, Pattern: XX, Action: nil}},
			`Invalid`:   {{Name: `Z0`, Pattern: Z0, Action: nil}},
			`Printable`: {{Name: `AA`, Pattern: AA, Action: nil}},
			`Escaped`:   {{Name: `EE`, Pattern: EE, Action: nil}},
			`Wildcard`: {
				lexer.Include(`Printable`),
				lexer.Include(`Escaped`),
			},

			`Root`: {
				lexer.Include(`Global`),

				{Name: `RS`, Pattern: RS, Action: nil},

				{Name: `NS`, Pattern: NS, Action: nil},

				{Name: `CO`, Pattern: `<`, Action: lexer.Push(`Composites`)},
				{Name: `PO`, Pattern: `\(`, Action: lexer.Push(`Parameters`)},
				{Name: `SO`, Pattern: `{`, Action: lexer.Push(`Statements`)},
			},

			`Composites`: {
				lexer.Include(`Global`),

				{Name: `CC`, Pattern: `>`, Action: lexer.Pop()},

				{Name: `FS`, Pattern: FS, Action: nil},

				{Name: `QQ`, Pattern: QQ, Action: nil},
				{Name: `NS`, Pattern: NS, Action: nil},
			},

			`Parameters`: {
				lexer.Include(`Global`),

				{Name: `PC`, Pattern: `\)`, Action: lexer.Pop()},

				{Name: `FS`, Pattern: FS, Action: nil},

				{Name: `QQ`, Pattern: QQ, Action: nil},
				{Name: `NU`, Pattern: NU, Action: nil},
				{Name: `ID`, Pattern: ID, Action: nil},
			},

			`Statements`: {
				lexer.Include(`Global`),

				{Name: `SO`, Pattern: `{`, Action: lexer.Push(`Statements`)},
				{Name: `SC`, Pattern: `}`, Action: lexer.Pop()},

				{Name: `ID`, Pattern: ID, Action: nil},
				{Name: `OP`, Pattern: `=`, Action: nil},
				{Name: `EX`, Pattern: EX, Action: nil},
			},
		})
	},
)

//nolint:gochecknoglobals
var symbol = sync.OnceValue(
	func() func(token string) lexer.TokenType {
		sym := LexerGenerator().Symbols()

		return func(token string) lexer.TokenType {
			typ, ok := sym[token]
			if !ok {
				panic("invalid lexer symbol: " + strconv.Quote(token))
			}

			return typ
		}
	},
)

// statementTerminator is the reason for delimiting the end of a statement.
type terminate byte

const (
	unterminated terminate = iota
	atError
	atEOF
	atNL
	atRS
	atSC
)

// consume returns a function that discards all tokens from the given lexer
// whose type matches any provided symbol and returns before consuming the next
// token that does not match any provided symbol.
//
// EOF will never be consumed even if it is one of the given symbols.
// The returned function will return immediately before consuming EOF and
// returns whether more tokens are available (next token is not EOF).
func consume(lex *lexer.PeekingLexer, sym ...string) func() (eof bool) {
	table := make(map[lexer.TokenType]struct{}, len(sym))

	for _, s := range sym {
		table[symbol()(s)] = struct{}{}
	}

	halt := func(tok *lexer.Token) bool {
		if tok == nil || tok.EOF() {
			return true
		}

		_, discard := table[tok.Type]

		return !discard
	}

	return func() bool {
		for !halt(lex.Peek()) {
			_ = lex.Next() // Discard the token.
		}

		return !lex.Peek().EOF()
	}
}

// AST is the root node of the parsed configuration file.
//
// It consumes tokens from the `Root` state of the lexer.
type AST struct {
	Pos    lexer.Position // Pos records the start position of the node.
	EndPos lexer.Position // EndPos records the end position of the node.
	Tokens []lexer.Token  // Tokens records the tokens consumed by the node

	Defs *Namespaces `parser:"@@"`
}

// Options are the default participle options used to build the parser.
//
//nolint:gochecknoglobals
var Options = []participle.Option{participle.Lexer(ConfigLexer)}

// ParseOptions are the default participle parse options used to parse the AST.
//
//nolint:gochecknoglobals
var ParseOptions = []participle.ParseOption{participle.AllowTrailing(true)}

func TraceOptions(w io.Writer) []participle.ParseOption {
	opt := make([]participle.ParseOption, len(ParseOptions)+1)

	copy(opt, ParseOptions)
	opt[len(ParseOptions)] = participle.Trace(w)

	return opt
}

// build returns a singleton parser for the AST type.
//
//nolint:gochecknoglobals
var build = sync.OnceValue(
	func() *participle.Parser[AST] {
		return participle.MustBuild[AST](Options...)
	},
)

// EBNF returns the parser grammar as a string in Extended Backus-Naur Form.
func EBNF() string { return build().String() }

// Make returns a function that parses an [AST] from the provided [io.Reader].
func Make(r io.Reader) func() (*AST, error) {
	return func() (*AST, error) {
		ast, err := build().Parse(pkg.Name, r, ParseOptions...)
		if err != nil {
			return nil, err
		}

		return ast, nil
	}
}
